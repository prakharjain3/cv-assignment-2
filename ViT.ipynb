{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"MNIST\"\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans #, MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    def __init__(self, patch_size) -> None:\n",
    "        super(Patchify, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, c, h, w = x.shape\n",
    "        \n",
    "        assert c == 1, \"Input must be a single channel image\"\n",
    "        assert h == w, \"Input must be a square image\"\n",
    "        assert h % self.patch_size == 0, \"Height must be divisible by patch size\"\n",
    "        \n",
    "        x = self.unfold(x)\n",
    "        \n",
    "        x = x.view(bs, c, self.patch_size, self.patch_size, -1).permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 49 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = datasets.MNIST(DATASET_PATH, train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_loader = DataLoader(mnist, batch_size=1, shuffle=True)\n",
    "\n",
    "for images, labels in mnist_loader:\n",
    "    \n",
    "    # plot the original image\n",
    "    plt.imshow(images[0, 0].squeeze(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    patchfify = Patchify(4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        patches = patchfify(images)\n",
    "    \n",
    "    \n",
    "    # calculate the number of patches\n",
    "    n_patches = patches.shape[1]\n",
    "    \n",
    "    # calculate the number of rows and columns for the subplots\n",
    "    n_rows = int(np.sqrt(n_patches))\n",
    "    n_cols = n_patches // n_rows\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "    \n",
    "        # reduce the gap between subplots\n",
    "    # plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax[i, j].imshow(patches[0, i * n_cols + j, 0].squeeze(), cmap=\"gray\")\n",
    "            ax[i, j].axis(\"off\")\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size, hidden_dim, num_classes, heads, mlp_dim, depth, n_patches, dropout, channels = 1) -> None:\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.depth = depth\n",
    "        self.n_patches = n_patches\n",
    "        self.channels = channels\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.Patcher = Patchify(self.patch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Linear transformation for the patches\n",
    "        self.linear_projection = nn.Linear(self.patch_size * self.patch_size * self.channels, hidden_dim)\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "                                                                # plus 1 for the class token\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, n_patches + 1, hidden_dim))\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=self.hidden_dim,\n",
    "                nhead=self.heads,\n",
    "                dim_feedforward=self.mlp_dim,\n",
    "                dropout=self.dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, images):\n",
    "        \n",
    "        patches = self.Patcher(images)\n",
    "        \n",
    "       \n",
    "        bs, n_patches = patches.shape[:2]\n",
    "        \n",
    "        patches = patches.view(bs, n_patches, -1)\n",
    "        \n",
    "        tokens = self.linear_projection(patches)\n",
    "        \n",
    "        class_tokens = self.class_token.repeat(bs, 1, 1)\n",
    "        \n",
    "        tokens = torch.cat([class_tokens, tokens], dim=1)\n",
    "        \n",
    "        tokens += self.positional_embedding[:, :(n_patches + 1)]\n",
    "        \n",
    "        encoded_tokens = self.transformer(tokens)\n",
    "        \n",
    "        class_token_output = encoded_tokens[:, 0]\n",
    "        \n",
    "        output = self.fc(class_token_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.MNIST(DATASET_PATH, train=True, download=True, transform=transform)\n",
    "    test_set = datasets.MNIST(DATASET_PATH, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_optimizer(opt, model, learning_rate)-> optim.Optimizer:\n",
    "    if opt == \"adam\":\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif opt == \"sgd\":\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif opt == \"rmsprop\":\n",
    "        return optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    elif opt == \"adagrad\":\n",
    "        return optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer\")\n",
    "\n",
    "def init_model_and_opt(device, optimizer, lr, patch_size, hidden_dim, num_classes, heads, mlp_dim, depth, n_patches, dropout, channels):\n",
    "    model = ViT(patch_size, hidden_dim, num_classes, heads, mlp_dim, depth, n_patches, dropout, channels).to(device)\n",
    "    optimizer = get_optimizer(optimizer, model, lr)\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "def train(model, optimizer, train_loader, device, epoch):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {train_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in tqdm(test_loader, desc = \"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            output = model(images)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f} \\n\")\n",
    "    \n",
    "    return test_loss, accuracy\n",
    "\n",
    "def plot_metrics(train_loss_list, test_loss_list, train_accuracy_list, test_accuracy_list, title):\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    ax[0, 0].plot(train_loss_list, label=\"Train Loss\")\n",
    "    ax[0, 0].set_title(\"Train Loss\")\n",
    "    ax[0, 0].set_xlabel(\"Epochs\")\n",
    "    ax[0, 0].set_ylabel(\"Loss\")\n",
    "    ax[0, 0].legend()\n",
    "\n",
    "    ax[0, 1].plot(test_loss_list, label=\"Test Loss\")\n",
    "    ax[0, 1].set_title(\"Test Loss\")\n",
    "    ax[0, 1].set_xlabel(\"Epochs\")\n",
    "    ax[0, 1].set_ylabel(\"Loss\")\n",
    "    ax[0, 1].legend()\n",
    "\n",
    "    ax[1, 0].plot(train_accuracy_list, label=\"Train Accuracy\")\n",
    "    ax[1, 0].set_title(\"Train Accuracy\")\n",
    "    ax[1, 0].set_xlabel(\"Epochs\")\n",
    "    ax[1, 0].set_ylabel(\"Accuracy\")\n",
    "    ax[1, 0].legend()\n",
    "\n",
    "    ax[1, 1].plot(test_accuracy_list, label=\"Test Accuracy\")\n",
    "    ax[1, 1].set_title(\"Test Accuracy\")\n",
    "    ax[1, 1].set_xlabel(\"Epochs\")\n",
    "    ax[1, 1].set_ylabel(\"Accuracy\")\n",
    "    ax[1, 1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Loading data\n",
    "    train_loader, test_loader = load_data()\n",
    "    \n",
    "    # get height and width of the images\n",
    "    HEIGHT, WIDTH = train_loader.dataset.data.shape[1:]\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    \n",
    "    patch_size = 4\n",
    "    hidden_dim = 256\n",
    "    num_classes = 10\n",
    "    heads = 8\n",
    "    mlp_dim = 512\n",
    "    depth = 2\n",
    "    n_patches = (HEIGHT // patch_size) ** 2\n",
    "    channels = 1\n",
    "    dropout = 0.1\n",
    "    lr = 0.001\n",
    "    \n",
    "    model, optimizer = init_model_and_opt(device, \"adam\", lr, patch_size, hidden_dim, num_classes, heads, mlp_dim, depth, n_patches, dropout, channels)\n",
    "    \n",
    "    # Number of epochs\n",
    "    epochs = 5\n",
    "\n",
    "    # Training and testing the model\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = train(model, optimizer, train_loader, device, epoch)\n",
    "        test_loss, test_accuracy = test(model, device, test_loader, epoch)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies, \"ViT Model with MNIST Dataset, Adam Optimizer, Learning Rate = 0.001, 5 Epochs, Patch Size = 4, Hidden Dim = 256, Heads = 8, MLP Dim = 512, Depth = 6, Dropout = 0.1, Channels = 1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faizal Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patch_size,\n",
    "                 input_dim, \n",
    "                 heads, \n",
    "                 mlp_dim, \n",
    "                 dropout, \n",
    "                 depth,\n",
    "                 num_patches):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout \n",
    "        self.depth = depth  \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        dim = 256\n",
    "        \n",
    "        self.patch_embedding = nn.Conv2d(1, dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=self.input_dim,\n",
    "                    nhead=self.heads,\n",
    "                    dim_feedforward=self.mlp_dim,\n",
    "                    dropout=self.dropout\n",
    "                ),\n",
    "                num_layers=self.depth\n",
    "            )\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: bs, 1, 28, 28\n",
    "        \n",
    "        x = self.patch_embedding(x) # bs, 256, 7, 7\n",
    "        x = x.flatten(2).transpose(1, 2) # bs, 49, 256\n",
    "        \n",
    "        x = x + self.positional_embedding # bs, 49, 256\n",
    "        \n",
    "        x = self.encoder(x) # bs, 49, 256\n",
    "        x = x.mean(dim=1) # bs, 256\n",
    "        \n",
    "        x = self.fc1(x) # bs, 10\n",
    "        \n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "patch_size = 4\n",
    "num_classes = 10\n",
    "input_dim = 256\n",
    "depth = 4\n",
    "heads = 4\n",
    "mlp_dim = 256\n",
    "dropout = 0.2\n",
    "num_patches = int((image_size * image_size)/(patch_size * patch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VIT(patch_size,\n",
    "            input_dim, \n",
    "            heads, \n",
    "            mlp_dim, \n",
    "            dropout, \n",
    "            depth,\n",
    "            num_patches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
